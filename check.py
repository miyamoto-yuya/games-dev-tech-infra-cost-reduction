import boto3
import argparse
from collections import defaultdict
from datetime import datetime, timedelta, timezone

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç®¡ç†
_session = None

def get_session():
    """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    global _session
    if _session is None:
        _session = boto3.Session()
    return _session

def set_profile(profile_name):
    """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨­å®šã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    global _session
    _session = boto3.Session(profile_name=profile_name)
    print(f"Using AWS profile: {profile_name}")

def get_client(service_name):
    """æŒ‡å®šã•ã‚ŒãŸã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
    return get_session().client(service_name)

# CloudWatchã‹ã‚‰æœ€å¤§CPUä½¿ç”¨ç‡ã‚’å–å¾—ï¼ˆ30æ—¥é–“ã€5åˆ†å¹³å‡ & 5åˆ†æœ€å¤§ï¼‰
def get_max_cpu_utilization(instance_id, namespace='AWS/EC2', dimension_name='InstanceId'):
    cloudwatch = get_client('cloudwatch')

    period = 300  # 5åˆ†ã®æœŸé–“
    days = 30  # å–å¾—ã™ã‚‹æœŸé–“ï¼ˆ30æ—¥ï¼‰

    end_time = datetime.now(timezone.utc)
    start_time = end_time - timedelta(days=days)
    
    # 5åˆ†å¹³å‡ã®æœ€å¤§å€¤
    max_avg_cpu = 0.0
    max_avg_timestamp = None
    
    # 5åˆ†æœ€å¤§ã®æœ€å¤§å€¤
    max_max_cpu = 0.0
    max_max_timestamp = None
    
    interval = timedelta(days=5)
    current_start = start_time

    while current_start < end_time:
        current_end = min(current_start + interval, end_time)

        response = cloudwatch.get_metric_statistics(
            Namespace=namespace,
            MetricName='CPUUtilization',
            Dimensions=[{'Name': dimension_name, 'Value': instance_id}],
            StartTime=current_start,
            EndTime=current_end,
            Period=period,
            Statistics=['Average', 'Maximum'],  # å¹³å‡ã¨æœ€å¤§ã®ä¸¡æ–¹ã‚’å–å¾—
            Unit='Percent'
        )

        datapoints = response.get('Datapoints', [])

        for dp in datapoints:
            # 5åˆ†å¹³å‡ã®æœ€å¤§å€¤ã‚’è¿½è·¡
            if dp.get('Average', 0) > max_avg_cpu:
                max_avg_cpu = dp['Average']
                max_avg_timestamp = dp['Timestamp']
            
            # 5åˆ†æœ€å¤§ã®æœ€å¤§å€¤ã‚’è¿½è·¡
            if dp.get('Maximum', 0) > max_max_cpu:
                max_max_cpu = dp['Maximum']
                max_max_timestamp = dp['Timestamp']

        current_start = current_end
    
    return {
        'avg': (round(max_avg_cpu, 2), max_avg_timestamp) if max_avg_cpu > 0 else (None, None),
        'max': (round(max_max_cpu, 2), max_max_timestamp) if max_max_cpu > 0 else (None, None)
    }

def get_ec2_instances():
    ec2 = get_client("ec2")
    
    response = ec2.describe_instances()
    instances_info = []
    instance_data = defaultdict(lambda: {"count": 0, "ebs_info": set(), "instance_ids": [], "auto_scaling_group": None})

    for reservation in response["Reservations"]:
        for instance in reservation["Instances"]:
            # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒçµ‚äº†æ¸ˆã¿ã®å ´åˆã€ã‚¹ã‚­ãƒƒãƒ—
            if instance["State"]["Name"] == "terminated":
                continue

            if instance["State"]["Name"] == "stopped":
                continue
            
            instance_id = instance["InstanceId"]
            instance_type = instance["InstanceType"]
            
            # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åã®å–å¾—
            instance_name = "N/A"
            for tag in instance.get("Tags", []):
                if tag["Key"] == "Name":
                    instance_name = tag["Value"]
                    break
            
            # AutoScalingã‚°ãƒ«ãƒ¼ãƒ—åã®å–å¾—ï¼ˆã‚ã‚‹å ´åˆï¼‰
            auto_scaling_group_name = None
            for tag in instance.get("Tags", []):
                if tag["Key"] == "aws:autoscaling:groupName":
                    auto_scaling_group_name = tag["Value"]
                    break
            
            # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã”ã¨ã«é›†è¨ˆ
            key = (instance_name, instance_type)
            instance_data[key]["count"] += 1
            instance_data[key]["instance_ids"].append(instance_id)  # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹IDã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
            instance_data[key]["auto_scaling_group"] = auto_scaling_group_name  # Auto Scaling ã‚°ãƒ«ãƒ¼ãƒ—åã‚’è¿½åŠ 
            
            # EBSæƒ…å ±ã®å–å¾—
            for block_device in instance.get("BlockDeviceMappings", []):
                volume_id = block_device.get("Ebs", {}).get("VolumeId", "N/A")
                if volume_id != "N/A":
                    volume = ec2.describe_volumes(VolumeIds=[volume_id])["Volumes"][0]
                    ebs_type = volume["VolumeType"]
                    storage_size = volume["Size"]
                    instance_data[key]["ebs_info"].add((ebs_type, storage_size))  # setã§é‡è¤‡é˜²æ­¢
    
    # çµæœã®æ•´ç† + CPUä½¿ç”¨ç‡ã®å–å¾—
    for (instance_name, instance_type), data in instance_data.items():
        count = data["count"]
        ebs_info = data["ebs_info"]
        instance_ids = data["instance_ids"]
        auto_scaling_group_name = data["auto_scaling_group"]

        # è¡¨ç¤ºã™ã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹IDã®è¨­å®š
        instance_id_display = instance_ids[0] if count == 1 else None

        # CPUä½¿ç”¨ç‡ã‚’å–å¾—ï¼ˆavg: 5åˆ†å¹³å‡ã®æœ€å¤§, max: 5åˆ†æœ€å¤§ã®æœ€å¤§ï¼‰
        cpu_avg, cpu_avg_ts = None, None
        cpu_max, cpu_max_ts = None, None

        if auto_scaling_group_name:
            result = get_max_cpu_utilization(auto_scaling_group_name, namespace='AWS/EC2', dimension_name='AutoScalingGroupName')
            cpu_avg, cpu_avg_ts = result['avg']
            cpu_max, cpu_max_ts = result['max']
        elif instance_ids:
            for iid in instance_ids:
                result = get_max_cpu_utilization(iid)
                avg_cpu, avg_ts = result['avg']
                max_cpu, max_ts = result['max']
                if avg_cpu is not None and (cpu_avg is None or avg_cpu > cpu_avg):
                    cpu_avg, cpu_avg_ts = avg_cpu, avg_ts
                if max_cpu is not None and (cpu_max is None or max_cpu > cpu_max):
                    cpu_max, cpu_max_ts = max_cpu, max_ts

        if ebs_info:
            for ebs in ebs_info:
                instances_info.append([
                    instance_name,
                    instance_id_display,
                    instance_type,
                    count,
                    ebs[0],
                    ebs[1],
                    cpu_avg,  # 5åˆ†å¹³å‡ã®æœ€å¤§
                    cpu_max,  # 5åˆ†æœ€å¤§ã®æœ€å¤§
                    cpu_avg_ts.isoformat() if cpu_avg_ts else "N/A"
                ])

    return instances_info


def get_rds_clusters():
    rds = get_client("rds")
    clusters_info = []

    # RDS ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æƒ…å ±ã‚’å–å¾—
    response = rds.describe_db_clusters()
    cluster_instance_ids = set()

    for cluster in response["DBClusters"]:
        if cluster["Engine"] == "docdb":
            continue
        cluster_name = cluster["DBClusterIdentifier"]
        node_count = len(cluster["DBClusterMembers"])

        instance_types = set()
        for member in cluster["DBClusterMembers"]:
            db_instance_identifier = member["DBInstanceIdentifier"]
            cluster_instance_ids.add(db_instance_identifier)
            db_instance = rds.describe_db_instances(DBInstanceIdentifier=db_instance_identifier)["DBInstances"][0]
            instance_type = db_instance["DBInstanceClass"]
            instance_types.add(instance_type)

        instance_type_display = ", ".join(sorted(instance_types)) if len(instance_types) > 1 else next(iter(instance_types))
        result = get_max_cpu_utilization(cluster_name, namespace='AWS/RDS', dimension_name='DBClusterIdentifier')
        cpu_avg, cpu_avg_ts = result['avg']
        cpu_max, _ = result['max']

        clusters_info.append([cluster_name, instance_type_display, node_count, cpu_avg, cpu_max, cpu_avg_ts.isoformat() if cpu_avg_ts else None])

    # å˜ä½“ã®RDSã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æƒ…å ±ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å±ã—ã¦ã„ãªã„ã‚‚ã®ï¼‰ã‚’å–å¾—
    response = rds.describe_db_instances()
    for instance in response["DBInstances"]:
        instance_id = instance["DBInstanceIdentifier"]
        if instance_id in cluster_instance_ids:
            continue  # æ—¢ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§å‡¦ç†æ¸ˆã¿ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—

        if instance["Engine"] == "docdb":
            continue

        instance_type = instance["DBInstanceClass"]
        result = get_max_cpu_utilization(instance_id, namespace='AWS/RDS', dimension_name='DBInstanceIdentifier')
        cpu_avg, cpu_avg_ts = result['avg']
        cpu_max, _ = result['max']
        clusters_info.append([instance_id, instance_type, 1, cpu_avg, cpu_max, cpu_avg_ts.isoformat() if cpu_avg_ts else None])

    return clusters_info

def get_docdb_clusters():
    docdb = get_client("docdb")
    response = docdb.describe_db_clusters()
    clusters_info = []
    
    for cluster in response["DBClusters"]:
        if cluster["Engine"] != "docdb":
            continue
        cluster_name = cluster["DBClusterIdentifier"]
        
        instance_types = set()
        for member in cluster["DBClusterMembers"]:
            db_instance_identifier = member["DBInstanceIdentifier"]
            db_instance = docdb.describe_db_instances(DBInstanceIdentifier=db_instance_identifier)["DBInstances"][0]
            instance_type = db_instance["DBInstanceClass"]
            instance_types.add(instance_type)
        
        if len(instance_types) > 1:
            instance_type_display = ", ".join(sorted(instance_types))
        else:
            instance_type_display = next(iter(instance_types))
        
        node_count = len(cluster["DBClusterMembers"])
        result = get_max_cpu_utilization(cluster_name, namespace='AWS/DocDB', dimension_name='DBClusterIdentifier')
        cpu_avg, cpu_avg_ts = result['avg']
        cpu_max, _ = result['max']
        clusters_info.append([cluster_name, instance_type_display, node_count, cpu_avg, cpu_max, cpu_avg_ts.isoformat() if cpu_avg_ts else None])
    
    return clusters_info

def get_redis_clusters():
    elasticache = get_client("elasticache")
    response = elasticache.describe_replication_groups()
    clusters_info = []

    for cluster in response["ReplicationGroups"]:
        cluster_name = cluster["ReplicationGroupId"]
        instance_type = cluster["CacheNodeType"]
        node_count = len(cluster["MemberClusters"])

        cpu_avg, cpu_avg_ts = None, None
        cpu_max = None
        for node_id in cluster["MemberClusters"]:
            result = get_max_cpu_utilization(
                node_id,
                namespace='AWS/ElastiCache',
                dimension_name='CacheClusterId'
            )
            avg, ts = result['avg']
            mx, _ = result['max']
            if avg is not None and (cpu_avg is None or avg > cpu_avg):
                cpu_avg, cpu_avg_ts = avg, ts
            if mx is not None and (cpu_max is None or mx > cpu_max):
                cpu_max = mx

        clusters_info.append([
            cluster_name,
            instance_type,
            node_count,
            cpu_avg,
            cpu_max,
            cpu_avg_ts.isoformat() if cpu_avg_ts else None
        ])

    return clusters_info

def get_memcache_clusters():
    elasticache = get_client("elasticache")
    response = elasticache.describe_cache_clusters()
    clusters_info = []

    for cluster in response["CacheClusters"]:
        # Memcachedã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
        if cluster["Engine"] != "memcached":
            continue
            
        cluster_name = cluster["CacheClusterId"]
        instance_type = cluster["CacheNodeType"]
        node_count = cluster["NumCacheNodes"]
        
        # CPUä½¿ç”¨ç‡ã‚’å–å¾—
        result = get_max_cpu_utilization(
            cluster_name,
            namespace='AWS/ElastiCache',
            dimension_name='CacheClusterId'
        )
        cpu_avg, cpu_avg_ts = result['avg']
        cpu_max, _ = result['max']

        clusters_info.append([
            cluster_name,
            instance_type,
            node_count,
            cpu_avg,
            cpu_max,
            cpu_avg_ts.isoformat() if cpu_avg_ts else None
        ])

    return clusters_info

import sys
import json
import io
import base64
import webbrowser
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError
from urllib.parse import quote

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆURL
DEFAULT_UPLOAD_URL = "https://oprto2mpbwtacfhzaql7phb5ay0rxifk.lambda-url.ap-northeast-1.on.aws/"

def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(
        description='AWS ã‚¤ãƒ³ãƒ•ãƒ©ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ä¾‹:
  # ğŸš€ æ¨å¥¨: ãƒ–ãƒ©ã‚¦ã‚¶ã§çµæœã‚’è¡¨ç¤º
  python check.py --profile account-a --open
  
  # ãƒ–ãƒ©ã‚¦ã‚¶è¡¨ç¤º + AIåˆ†æ
  python check.py --profile account-a --open --analyze
  
  # æ¨™æº–å‡ºåŠ›ã«å‡ºåŠ›ï¼ˆã‚³ãƒ”ãƒšç”¨ï¼‰
  python check.py --profile account-a --stdout
  
  # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
  python check.py --profile account-a --output result-a.txt
  
  # è¤‡æ•°ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä¸€æ‹¬å‡¦ç†
  for p in account-a account-b account-c; do
    python check.py --profile $p --open --analyze
  done
'''
    )
    parser.add_argument(
        '--profile', '-p',
        type=str,
        default=None,
        help='ä½¿ç”¨ã™ã‚‹AWSãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: --profile ii-dev)'
    )
    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: --output result.txt)'
    )
    parser.add_argument(
        '--stdout', '-s',
        action='store_true',
        help='çµæœã‚’æ¨™æº–å‡ºåŠ›ã«å‡ºåŠ›ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ãªãã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¡¨ç¤ºï¼‰'
    )
    parser.add_argument(
        '--upload', '-u',
        action='store_true',
        help='çµæœã‚’è‡ªå‹•çš„ã«ã‚µãƒ¼ãƒãƒ¼ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰'
    )
    parser.add_argument(
        '--open', '-O',
        action='store_true',
        dest='open_browser',
        help='çµæœã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§è‡ªå‹•çš„ã«é–‹ãï¼ˆæ¨å¥¨ï¼‰'
    )
    parser.add_argument(
        '--analyze', '-a',
        action='store_true',
        help='AIåˆ†æã‚‚å®Ÿè¡Œ'
    )
    parser.add_argument(
        '--url',
        type=str,
        default=DEFAULT_UPLOAD_URL,
        help=f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆURL (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_UPLOAD_URL})'
    )
    parser.add_argument(
        '--region', '-r',
        type=str,
        default=None,
        help='AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (ä¾‹: --region ap-northeast-1)'
    )
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éè¡¨ç¤ºã«ã™ã‚‹'
    )
    return parser.parse_args()


def open_in_browser(url: str, resource_text: str, analysis: str = None, token_usage: dict = None, profile: str = None):
    """çµæœã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã"""
    # ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    data = {
        "resources": resource_text,
        "analysis": analysis,
        "token_usage": token_usage,
        "profile": profile or "unknown",
        "timestamp": __import__('datetime').datetime.now().isoformat()
    }
    
    # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    json_str = json.dumps(data, ensure_ascii=False)
    encoded = base64.b64encode(json_str.encode('utf-8')).decode('ascii')
    
    # URLãƒãƒƒã‚·ãƒ¥ã¨ã—ã¦è¿½åŠ 
    full_url = f"{url.rstrip('/')}/#data={encoded}"
    
    # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ãï¼ˆå¤±æ•—ã—ã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ã—ãªã„ï¼‰
    browser_opened = False
    try:
        browser_opened = webbrowser.open(full_url)
    except Exception:
        pass
    
    # URLã‚’å¸¸ã«è¡¨ç¤ºï¼ˆã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒªãƒ³ã‚¯ã¨ã—ã¦ï¼‰
    print(f"\n{'='*60}", file=sys.stderr)
    print(f"ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã:", file=sys.stderr)
    print(f"{'='*60}", file=sys.stderr)
    print(f"\n{full_url}\n", file=sys.stderr)
    print(f"{'='*60}", file=sys.stderr)
    
    if not browser_opened:
        print("ğŸ’¡ ä¸Šè¨˜URLã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã«ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆã—ã¦ãã ã•ã„", file=sys.stderr)
    
    return full_url


def upload_results(resource_text: str, url: str, analyze: bool = False) -> dict:
    """åé›†çµæœã‚’ã‚µãƒ¼ãƒãƒ¼ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    payload = {
        "action": "analyze_text" if analyze else "upload_only",
        "resource_text": resource_text
    }
    
    data = json.dumps(payload).encode('utf-8')
    headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json'
    }
    
    req = Request(url, data=data, headers=headers, method='POST')
    
    try:
        with urlopen(req, timeout=120) as response:
            result = json.loads(response.read().decode('utf-8'))
            return result
    except HTTPError as e:
        return {"error": f"HTTP Error {e.code}: {e.reason}"}
    except URLError as e:
        return {"error": f"URL Error: {e.reason}"}
    except Exception as e:
        return {"error": str(e)}


def output_results(ec2_instances, rds_clusters, docdb_clusters, redis_clusters, memcache_clusters, file=None):
    """åé›†çµæœã‚’å‡ºåŠ›"""
    out = file or sys.stdout
    
    print("\nEC2 :", file=out)
    print("Instance Name\tInstance ID\tInstance Type\tå°æ•°\tEBS Type\tEBS Size\tCPU AvgMax\tCPU Max\tTimestamp", file=out)
    for instance in ec2_instances:
        print("\t".join(map(str, instance)), file=out)

    print("\nRedis :", file=out)
    print("Cluster Name\tInstance Type\tå°æ•°\tCPU AvgMax\tCPU Max\tTimestamp", file=out)
    for cluster in redis_clusters:
        print("\t".join(map(str, cluster)), file=out)

    print("\nMemcached :", file=out)
    print("Cluster Name\tInstance Type\tå°æ•°\tCPU AvgMax\tCPU Max\tTimestamp", file=out)
    for cluster in memcache_clusters:
        print("\t".join(map(str, cluster)), file=out)

    print("\nRDS :", file=out)
    print("Cluster Name\tInstance Type\tå°æ•°\tCPU AvgMax\tCPU Max\tTimestamp", file=out)
    for cluster in rds_clusters:
        print("\t".join(map(str, cluster)), file=out)

    print("\nDocumentDB :", file=out)
    print("Cluster Name\tInstance Type\tå°æ•°\tCPU AvgMax\tCPU Max\tTimestamp", file=out)
    for cluster in docdb_clusters:
        print("\t".join(map(str, cluster)), file=out)


def log(msg, quiet=False):
    """é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›ï¼ˆquietãƒ¢ãƒ¼ãƒ‰ã§ãªã„å ´åˆã®ã¿ï¼‰"""
    if not quiet:
        print(msg, file=sys.stderr)


def get_result_text(ec2_instances, rds_clusters, docdb_clusters, redis_clusters, memcache_clusters):
    """åé›†çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å–å¾—"""
    buffer = io.StringIO()
    output_results(ec2_instances, rds_clusters, docdb_clusters, redis_clusters, memcache_clusters, file=buffer)
    return buffer.getvalue()


def format_analysis(analysis: str) -> str:
    """AIåˆ†æçµæœã‚’è¦‹ã‚„ã™ããƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    separator = "=" * 60
    return f"""
{separator}
ğŸ¤– AI ã‚µã‚¤ã‚¸ãƒ³ã‚°ææ¡ˆ
{separator}

{analysis}

{separator}
"""


def main():
    args = parse_args()
    quiet = args.quiet or args.stdout  # stdoutå‡ºåŠ›æ™‚ã¯è‡ªå‹•çš„ã«quiet
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨­å®š
    if args.profile:
        global _session
        if args.region:
            _session = boto3.Session(profile_name=args.profile, region_name=args.region)
            log(f"Using AWS profile: {args.profile}, region: {args.region}", quiet)
        else:
            _session = boto3.Session(profile_name=args.profile)
            log(f"Using AWS profile: {args.profile}", quiet)
    elif args.region:
        _session = boto3.Session(region_name=args.region)
        log(f"Using AWS region: {args.region}", quiet)
    
    log("Collecting AWS resource information...", quiet)
    
    ec2_instances = get_ec2_instances()
    log(f"  EC2: {len(ec2_instances)} instances", quiet)
    
    rds_clusters = get_rds_clusters()
    log(f"  RDS: {len(rds_clusters)} clusters/instances", quiet)
    
    docdb_clusters = get_docdb_clusters()
    log(f"  DocumentDB: {len(docdb_clusters)} clusters", quiet)
    
    redis_clusters = get_redis_clusters()
    log(f"  Redis: {len(redis_clusters)} clusters", quiet)
    
    memcache_clusters = get_memcache_clusters()
    log(f"  Memcached: {len(memcache_clusters)} clusters", quiet)

    # çµæœãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    result_text = get_result_text(ec2_instances, rds_clusters, docdb_clusters, redis_clusters, memcache_clusters)

    # ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ããƒ¢ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰
    if args.open_browser:
        analysis = None
        token_usage = None
        
        if args.analyze:
            log("\nRequesting AI analysis...", quiet)
            response = upload_results(result_text, args.url, analyze=True)
            if "error" not in response and "analysis" in response:
                analysis = response["analysis"]
                token_usage = response.get("token_usage")
                log("âœ… AI analysis completed!", quiet)
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¡¨ç¤º
                if token_usage:
                    log(f"   ğŸ“Š Tokens: {token_usage.get('input_tokens', 0):,} in + {token_usage.get('output_tokens', 0):,} out = {token_usage.get('total_tokens', 0):,} total", quiet)
                    log(f"   ğŸ’° Cost: ${token_usage.get('total_cost_usd', 0):.6f} (ç´„{token_usage.get('total_cost_jpy', 0):.4f}å††)", quiet)
            else:
                log("âš ï¸ AI analysis failed, opening without analysis", quiet)
        
        open_in_browser(args.url, result_text, analysis, token_usage, args.profile)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚‚ä½µç”¨ã™ã‚‹å ´åˆ
        if args.output:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(result_text)
                if analysis:
                    f.write(format_analysis(analysis))
            log(f"   Also saved to: {args.output}", quiet)

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤ºï¼‰
    elif args.upload:
        log(f"\nUploading to: {args.url}", quiet)
        
        if args.analyze:
            log("Requesting AI analysis...", quiet)
        
        response = upload_results(result_text, args.url, analyze=args.analyze)
        
        if "error" in response:
            log(f"âŒ Upload failed: {response['error']}", quiet=False)
            sys.exit(1)
        else:
            log("âœ… Upload successful!", quiet)
            
            # AIåˆ†æçµæœãŒã‚ã‚Œã°è¡¨ç¤º
            if args.analyze and "analysis" in response:
                print(format_analysis(response["analysis"]))
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚‚ä½µç”¨ã™ã‚‹å ´åˆ
            if args.output:
                with open(args.output, "w", encoding="utf-8") as f:
                    f.write(result_text)
                    if args.analyze and "analysis" in response:
                        f.write(format_analysis(response["analysis"]))
                log(f"Output also saved to: {args.output}", quiet)
    
    # æ¨™æº–å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰
    elif args.stdout:
        output_results(ec2_instances, rds_clusters, docdb_clusters, redis_clusters, memcache_clusters)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰
    elif args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            output_results(ec2_instances, rds_clusters, docdb_clusters, redis_clusters, memcache_clusters, file=f)
        log(f"\nOutput saved to: {args.output}", quiet)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: output.txtã«å‡ºåŠ›
    else:
        output_file = 'output.txt'
        with open(output_file, "w", encoding="utf-8") as f:
            output_results(ec2_instances, rds_clusters, docdb_clusters, redis_clusters, memcache_clusters, file=f)
        log(f"\nOutput saved to: {output_file}", quiet)


if __name__ == "__main__":
    main()
